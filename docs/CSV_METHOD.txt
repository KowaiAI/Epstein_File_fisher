â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  â–‘â–’â–ˆâ–€â–€â–€â–‘â–‘â–€â–‘â–‘â–ˆâ–‘â–‘â–ˆâ–€â–€â–‘â–‘â–‘â–’â–ˆâ–€â–€â–€â–‘â–‘â–€â–‘â–‘â–ˆâ–€â–€â–‘â–ˆâ–‘â–‘â–‘â–‘â–ˆâ–€â–€â–‘â–ˆâ–€â–€â–„                â•‘
â•‘  â–‘â–’â–ˆâ–€â–€â–‘â–‘â–‘â–ˆâ–€â–‘â–ˆâ–‘â–‘â–ˆâ–€â–€â–‘â–‘â–‘â–’â–ˆâ–€â–€â–‘â–‘â–‘â–ˆâ–€â–‘â–€â–€â–„â–‘â–ˆâ–€â–€â–ˆâ–‘â–ˆâ–€â–€â–‘â–ˆâ–„â–„â–€                â•‘
â•‘  â–‘â–’â–ˆâ–‘â–‘â–‘â–‘â–€â–€â–€â–‘â–€â–€â–‘â–€â–€â–€â–‘â–‘â–‘â–’â–ˆâ–‘â–‘â–‘â–‘â–€â–€â–€â–‘â–€â–€â–€â–‘â–€â–‘â–‘â–€â–‘â–€â–€â–€â–‘â–€â–‘â–€â–€                â•‘
â•‘                                                                    â•‘
â•‘              CSV METHOD (RECOMMENDED - NO BOT ISSUES!)            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒŸ **RECOMMENDED METHOD** - Uses direct download links!

The CSV method downloads files using a CSV file with direct links.
This avoids bot detection and is MUCH more reliable!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHY USE THE CSV METHOD?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… NO bot detection issues
âœ… Direct download links
âœ… Much more reliable
âœ… Faster (no waiting for page loads)
âœ… Works every time
âœ… Same interactive menu
âœ… Same file organization

âŒ Web scraping has problems:
   - Bot detection blocks pagination
   - 403 Forbidden errors
   - Unreliable
   - Slow

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REQUIREMENTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•

You need the CSV file: master_file_links.csv

Location: /home/hibiscus/Downloads/master_file_links.csv

This CSV contains 575 direct download links for all 12 data sets.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HOW TO USE:
â•â•â•â•â•â•â•â•â•â•â•

OPTION 1: INTERACTIVE MENU (EASIEST)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Open Terminal/Command Prompt
2. Navigate to the folder:
   cd ~/Documents/Claude/scrapers/doj-epstein

3. Activate environment:
   source venv/bin/activate

4. Run CSV downloader:
   python csv_downloader.py

5. Select data sets from the menu!

OPTION 2: COMMAND LINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Download specific data sets:
  python csv_downloader.py --data-sets 1 2 3

Download just Data Set 8:
  python csv_downloader.py --data-sets 8

Download everything:
  python csv_downloader.py --data-sets 1 2 3 4 5 6 7 8 9 10 11 12

Custom CSV location:
  python csv_downloader.py /path/to/file.csv --data-sets 8

Custom output directory:
  python csv_downloader.py --output-dir /path/to/output --data-sets 1

Just collect metadata (no download):
  python csv_downloader.py --no-download

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHAT'S IN THE CSV?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The CSV has these columns:
- number: Sequential number
- url: Direct download link
- link_text: Filename (EFTA00000001.pdf)
- data_set: Which data set (1-12)

Total: 575 files across 12 data sets

Data Set Breakdown:
  Data Set 1:  50 files
  Data Set 2:  49 files
  Data Set 3:  49 files
  Data Set 4:  49 files
  Data Set 5:  49 files
  Data Set 6:  13 files
  Data Set 7:  17 files
  Data Set 8:  99 files  â† Largest!
  Data Set 9:  50 files
  Data Set 10: 50 files
  Data Set 11: 50 files
  Data Set 12: 50 files

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXAMPLES:
â•â•â•â•â•â•â•â•â•

# Download Data Set 8 (99 files):
python csv_downloader.py --data-sets 8

# Download Data Sets 1, 5, and 10:
python csv_downloader.py --data-sets 1 5 10

# Use different CSV file:
python csv_downloader.py ~/Desktop/links.csv --data-sets 1

# Save to external drive:
python csv_downloader.py --output-dir /mnt/external/Epstein --data-sets 8

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SPEED:
â•â•â•â•â•â•

Average: ~2 seconds per file
Data Set 8 (99 files): ~3-4 minutes
All files (575): ~20-25 minutes

Much faster than web scraping! No waiting for page loads.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMPARISON:
â•â•â•â•â•â•â•â•â•â•â•

WEB SCRAPER (scraper.py):
  âŒ Bot detection issues
  âŒ 403 Forbidden errors
  âŒ Unreliable pagination
  âŒ Slow (loads every page)
  âœ… Finds new files automatically

CSV DOWNLOADER (csv_downloader.py):
  âœ… No bot detection
  âœ… Direct links work perfectly
  âœ… Very reliable
  âœ… Fast downloads
  âŒ Need CSV file updated manually

**RECOMMENDATION: Use CSV downloader!**

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TROUBLESHOOTING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âŒ "CSV file not found"
   â†’ Make sure the path is correct
   â†’ Default: /home/hibiscus/Downloads/master_file_links.csv
   â†’ Or specify: python csv_downloader.py /path/to/file.csv

âŒ "No data sets found"
   â†’ CSV might be empty or corrupted
   â†’ Re-download the CSV file

âŒ Download fails
   â†’ Check internet connection
   â†’ Try again - it will skip already-downloaded files

âŒ Slow downloads
   â†’ Normal! Each file takes ~2 seconds
   â†’ This is intentional rate limiting

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FILES GO TO:
â•â•â•â•â•â•â•â•â•â•â•â•

Same location as web scraper:
  ~/Documents/Epstein/data_set_X/documents/

Example:
  ~/Documents/Epstein/data_set_8/documents/EFTA00009676.pdf

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHICH METHOD SHOULD I USE?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Use CSV downloader if:
  âœ… You have the CSV file
  âœ… You want reliable downloads
  âœ… You're downloading specific data sets
  âœ… You don't want bot detection issues

Use web scraper if:
  âœ… You don't have the CSV file
  âœ… You want to discover NEW files
  âœ… CSV is outdated
  âœ… You're willing to deal with bot detection

**99% of users should use the CSV downloader!**

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
